# Semantic Search with OpenSearch and Cohere

## Cluster Settings:
PUT /_cluster/settings
{
    "persistent": {
        "plugins.ml_commons.allow_registering_model_via_url": true,
        "plugins.ml_commons.only_run_on_ml_node": false,
        "plugins.ml_commons.connector_access_control_enabled": true,
        "plugins.ml_commons.model_access_control_enabled": true,
        "plugins.ml_commons.memory_feature_enabled": true,
        "plugins.ml_commons.rag_pipeline_feature_enabled": true,
        "plugins.ml_commons.trusted_connector_endpoints_regex": [
          "^https://api\\.cohere\\.ai/.*$",
          "^https://api\\.openai\\.com/.*$"
        ]
    }
}


# Create Model Group:
POST /_plugins/_ml/model_groups/_register
{
    "name": "Model_Group",
    "description": "Public ML Model Group",
    "access_mode": "public"
}
# MODEL_GROUP_ID: 


## Register and deploy the text embedding model
POST /_plugins/_ml/models/_register?deploy=true
{
    "name": "embed-english-v3.0",
    "function_name": "remote",
    "description": "Cohere model for embedding",
    "model_group_id": "<MODEL_GROUP_ID>",
    "connector": {
      "name": "Cohere Connector",
      "description": "Cohere model for Embeddings",
      "version": "1.0",
      "protocol": "http",
      "credential": {
             "cohere_key": "<COHERE_KEY>"
         },
      "parameters": {
        "model": "embed-english-v3.0",
        "truncate": "END"
      },
      "actions": [{
         "action_type": "predict",
         "method": "POST",
         "url": "https://api.cohere.ai/v1/embed",
         "headers": {
                 "Authorization": "Bearer ${credential.cohere_key}"
             },
  			"request_body": "{ \"texts\": ${parameters.texts}, \"truncate\": \"${parameters.truncate}\", \"model\": \"${parameters.model}\", \"input_type\": \"search_document\" }",
  			"pre_process_function": "connector.pre_process.cohere.embedding",
			  "post_process_function": "connector.post_process.cohere.embedding"
     }]
  }
}
# EMBEDDING_MODEL_ID:


## Create Ingestion Pipeline
PUT _ingest/pipeline/embedding-ingest-pipeline
{
  "description": "Neural Search Pipeline",
  "processors" : [
    {
      "text_embedding": {
        "model_id": "<EMBEDDING_MODEL_ID>",
        "field_map": {
          "content": "content_embedding"
        }
      }
    }
  ]
}



## Register and deploy the language model to the cluster:
POST /_plugins/_ml/models/_register?deploy=true
{
    "name": "gpt-3.5-turbo",
    "function_name": "remote",
    "description": "OpenAI Chat Connector",
    "model_group_id": "<MODEL_GROUP_ID>",
    "connector": {
      "name": "OpenAI Chat Connector",
      "description": "The connector to public OpenAI model service for GPT 3.5",
      "version": 1,
      "protocol": "http",
      "parameters": {
          "endpoint": "api.openai.com",
          "model": "gpt-3.5-turbo"
      },
      "credential": {
          "openAI_key": "<OPENAI_KEY>"
      },
      "actions": [
          {
              "action_type": "predict",
              "method": "POST",
              "url": "https://${parameters.endpoint}/v1/chat/completions",
              "headers": {
                  "Authorization": "Bearer ${credential.openAI_key}"
              },
              "request_body": "{ \"model\": \"${parameters.model}\", \"messages\": ${parameters.messages} }"
          }
      ]
  }
}
# LLM_MODEL_ID: 


## Put the RAG search pipeline in place
PUT _search/pipeline/rag-search-pipeline
{
  "phase_results_processors": [
    {
      "normalization-processor": {
        "normalization": {
          "technique": "min_max"
        },
        "combination": {
          "technique": "arithmetic_mean",
          "parameters": {
            "weights": [
              0.3,
              0.7
            ]
          }
        }
      }
    }
  ],
  "response_processors": [
    {
      "retrieval_augmented_generation": {
        "description": "RAG search pipeline to be used with Cohere index",
        "model_id": "<LLM_MODEL_ID>",
        "context_field_list": ["content"],
        "system_prompt": "You are a helpful OpenSearch assistant called DocBot",
        "user_instructions": "Answer the following question using only the provided context. If the provided context does not provide enough information to answer the question respond with something along the lines of 'I dont have enough information to answer that.'"
      }
    }
  ]
}



## Create KNN index. Note* need to match space to model space. eg embed-english-v3.0 recommends cosine similarity:
PUT /docbot
{
	"settings": {
		"index.knn": true,
		"default_pipeline": "embedding-ingest-pipeline",
    "index.search.default_pipeline": "rag-search-pipeline"
	},
	"mappings": {
		"properties": {
			"content_embedding": {
				"type": "knn_vector",
				"dimension": 1024,
				"method": {
					"name": "hnsw",
					"space_type": "cosinesimil",
					"engine": "nmslib"
				}
			},
			"content": {
				"type": "text"
			}
		}
	}
}



## Hydrate index with `_bulk`
POST _bulk
{ "create" : { "_index" : "docbot", "_id" : "1" }}
{ "content":"Testing neural search"}
{ "create" : { "_index" : "docbot", "_id" : "2" }}
{ "content": "What are we doing"}
{ "create" : { "_index" : "docbot", "_id" : "3" } }
{ "content": "This should exist"}


## Create a conversation 
POST /_plugins/_ml/memory/conversation
{
  "name": "DocBot Conversation"
}
# CONVERSATION_ID:



## Search

GET /docbot/_search
{
  "_source": {
    "exclude": [
      "content_embedding"
    ]
  },
  "query": {
    "hybrid": {
      "queries": [
        {
          "match": {
            "content": {
              "query": "How do I enable segment replication"
            }
          }
        },
        {
          "neural": {
            "content_embedding": {
              "query_text": "How do I enable segment replication",
              "model_id": "<EMBEDDING_MODEL_ID>",
              "k": 5
            }
          }
        }
      ]
    }
  },
  "ext": {
		"generative_qa_parameters": {
		  "llm_model": "gpt-3.5-turbo",
			"llm_question": "How do I enable segment replication",
			"conversation_id": "<CONVERSATION_ID>",
                         "context_size": 3,
                         "interaction_size": 3,
                         "timeout": 45
		}
	}
}


## Cleanup

POST /_plugins/_ml/models/<EMBEDDING_MODEL_ID>/_undeploy
DELETE /_plugins/_ml/models/<EMBEDDING_MODEL_ID>
POST /_plugins/_ml/models/<LLM_MODEL_ID>/_undeploy
DELETE /_plugins/_ml/models/<LLM_MODEL_ID>
DELETE _ingest/pipeline/embedding-ingest-pipeline
DELETE _search/pipeline/rag-search-pipeline
DELETE /_plugins/_ml/memory/conversation/<CONVERSATION_ID>
DELETE docbot


### Troubleshoot:

POST /_plugins/_ml/models/<MODEL_ID>/_predict
{
  "parameters": {
    "texts": ["This should exist"]
  }
} 



GET /docbot/_search?search_pipeline=_none
{
  "query": {
    "match_all": {}
  }
}

GET /docbot/_mapping

